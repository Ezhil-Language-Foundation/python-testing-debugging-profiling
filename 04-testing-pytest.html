<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="pandoc">
    <title>Software Carpentry: Testing</title>
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" type="text/css" href="css/bootstrap/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="css/bootstrap/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="css/swc.css" />
    <link rel="alternate" type="application/rss+xml" title="Software Carpentry Blog" href="http://software-carpentry.org/feed.xml"/>
    <meta charset="UTF-8" />
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body class="lesson">
    <div class="container card">
      <div class="banner">
        <a href="http://software-carpentry.org" title="Software Carpentry">
          <img alt="Software Carpentry banner" src="img/software-carpentry-banner.png" />
        </a>
      </div>
      <article>
      <div class="row">
        <div class="col-md-10 col-md-offset-1">
                    <a href="index.html"><h1 class="title">Testing</h1></a>
          <h2 class="subtitle">Running Tests with pytest</h2>
          <section class="objectives panel panel-warning">
<div class="panel-heading">
<h2><span class="glyphicon glyphicon-certificate"></span>Learning Objectives</h2>
</div>
<div class="panel-body">
<ul>
<li>Understand how to run a test suite using the pytest framework</li>
<li>Understand how to read the output of a pytest test suite</li>
</ul>
</div>
</section>
<p>We created a suite of tests for our rescale function, but it was annoying to run them one at a time. It would be a lot better if there were some way to run them all at once, just reporting which tests fail and which succeed.</p>
<p>Thankfully, that exists. Let us test the following function that we will put into a file <code>preprocess.py</code>:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> whiten(data):
    <span class="co">&quot;&quot;&quot;</span>
<span class="co">    Return a whitened copy of the data, i.e. data with zero mean and unit</span>
<span class="co">    variance.</span>

<span class="co">    Parameters</span>
<span class="co">    ----------</span>
<span class="co">    data : ndarray</span>
<span class="co">        The data to whiten.</span>

<span class="co">    Returns</span>
<span class="co">    -------</span>
<span class="co">    whitened : ndarray</span>
<span class="co">        The whitened data.</span>
<span class="co">    &quot;&quot;&quot;</span>
    centered = data - data.mean()
    whitened = centered / data.std()
    <span class="kw">return</span> whitened</code></pre>
<p>We will test this function in a file <code>test_whiten.py</code>:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> numpy.testing.utils <span class="ch">import</span> assert_allclose, assert_equal
<span class="ch">import</span> numpy

<span class="ch">from</span> preprocess <span class="ch">import</span> whiten

<span class="kw">def</span> test_1d():
    test_data = numpy.array([<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>])
    whitened = whiten(test_data)
    assert_allclose(whitened.mean(), <span class="dv">0</span>)
    assert_allclose(whitened.std(), <span class="dv">1</span>)
    assert_allclose(whitened*test_data.std() + test_data.mean(),
                    test_data)
<span class="kw">def</span> test_2d():
    test_data = numpy.array([[<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>],
                             [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">1</span>]])
    whitened = whiten(test_data)
    assert_allclose(whitened.mean(), <span class="dv">0</span>)
    assert_allclose(whitened.std(), <span class="dv">1</span>)
    assert_allclose(whitened*test_data.std() + test_data.mean(),
                    test_data)</code></pre>
<p>Now we can run the <code>pytest</code> utility in the directory where we stored the test file:</p>
<pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">py.test</span></code></pre>
<pre class="output"><code>================================= test session starts ==================================
platform linux -- Python 3.5.1, pytest-2.9.1, py-1.4.31, pluggy-0.3.1
rootdir: [...], inifile:
collected 2 items

test_whiten.py .F

======================================= FAILURES =======================================
_______________________________________ test_2d ________________________________________

    def test_2d():
        test_data = numpy.array([[1, 3, 15],
                                 [12, 3, 4]])
        whitened = whiten(test_data)
&gt;       assert_allclose(whitened.mean(), 0)
E       AssertionError:
E       Not equal to tolerance rtol=1e-07, atol=0
E       
E       (mismatch 100.0%)
E        x: array(6.47630097698008e-17)
E        y: array(0)

test_whiten.py:17: AssertionError
========================== 1 failed, 1 passed in 0.12 seconds ==========================</code></pre>
<p>In the above case, the pytest package ‘sniffed-out’ the tests in the directory and ran them together to produce a report of the sum of the files and functions matching having the name <code>test_*</code>.</p>
<p>The major boon a testing framework provides is exactly that, a utility to find and run the tests automatically. With <code>pytest</code>, this is the command-line tool called <em>py.test</em>. When <em>py.test</em> is run, it will search all the directories whose names start or end with the word <em>test</em>, find all of the Python modules in these directories whose names start or end with <em>test</em>, import them, and run all of the functions and classes whose names start with <em>test</em>. This automatic registration of test code saves tons of human time and allows us to focus on what is important: writing more tests.</p>
<p>When you run <em>py.test</em>, it will print a dot (<code>.</code>) on the screen for every test that passes, and an <code>F</code> for every test that fails. After the dots, <em>py.test</em> will print summary information.</p>
<aside class="callout panel panel-info">
<div class="panel-heading">
<h2><span class="glyphicon glyphicon-pushpin"></span>More information / less information</h2>
</div>
<div class="panel-body">
<p>To see more information, such as the name of every test function, use the <code>-v</code> (for “verbose”) command line argument. To see less, use <code>-q</code> (for “quiet”).</p>
</div>
</aside>
<p>In the above case, our failing test case is actually a result of a test that is too strict. While we have thought of using <code>assert_allclose</code> instead of a <code>==</code> comparison, it tells us that the result is not <code>equal to tolerance rtol=1e-07, atol=0</code>. Apparently, the default <em>absolute</em> tolerance is 0 and only a relative tolerance is used. Normally, this should be fine, but it is obviously very strict when comparing to zero. Adding <code>atol=1e-15</code> to our assertion should fix the test:</p>
<pre class="sourceCode python"><code class="sourceCode python">...
    assert_allclose(whitened.mean(), <span class="dv">0</span>, atol=<span class="fl">1e-15</span>)
...</code></pre>
<pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">py.test</span></code></pre>
<pre class="output"><code>================================= test session starts ==================================
platform linux -- Python 3.5.1, pytest-2.9.1, py-1.4.31, pluggy-0.3.1
rootdir: [...], inifile:
collected 2 items

test_whiten.py ..

=============================== 2 passed in 0.10 seconds ===============================</code></pre>
<section class="challenge panel panel-success">
<div class="panel-heading">
<h2><span class="glyphicon glyphicon-pencil"></span>Add more tests</h2>
</div>
<div class="panel-body">
<p>Add more tests to <code>test_whiten.py</code> and take care of the edge cases.</p>
</div>
</section>
<p>As we write more code, we would write more tests, and <em>py.test</em> would produce more dots. Each passing test is a small, satisfying reward for having written quality scientific software.</p>
        </div>
      </div>
      </article>
      <div class="footer">
        <a class="label swc-blue-bg" href="http://software-carpentry.org">Software Carpentry</a>
        <a class="label swc-blue-bg" href="https://github.com/paris-swc/python-testing-debugging-profiling">Source</a>
        <a class="label swc-blue-bg" href="mailto:admin@software-carpentry.org">Contact</a>
        <a class="label swc-blue-bg" href="LICENSE.html">License</a>
      </div>
    </div>
    <!-- Javascript placed at the end of the document so the pages load faster -->
    <script src="http://software-carpentry.org/v5/js/jquery-1.9.1.min.js"></script>
    <script src="css/bootstrap/bootstrap-js/bootstrap.js"></script>
    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
  </body>
</html>
